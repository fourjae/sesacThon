import os
import hdbscan
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import StandardScaler
import requests
import openai
import re
from dotenv import load_dotenv

# -----------------------
# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
# -----------------------
load_dotenv()
CLOVA_ID = os.getenv("CLOVA_CLIENT_ID")
CLOVA_SECRET = os.getenv("CLOVA_CLIENT_SECRET")
OPENAI_KEY = os.getenv("OPENAI_API_KEY")

openai.api_key = OPENAI_KEY


# -----------------------
# 1) CLOVA STT
# -----------------------
def clova_stt(audio_path: str) -> str:
    url = "https://naveropenapi.apigw.ntruss.com/recog/v1/stt?lang=Kor"

    headers = {
        "X-NCP-APIGW-API-KEY-ID": CLOVA_ID,
        "X-NCP-APIGW-API-KEY": CLOVA_SECRET,
        "Content-Type": "application/octet-stream",
    }

    with open(audio_path, "rb") as f:
        audio_data = f.read()

    response = requests.post(url, headers=headers, data=audio_data)
    result = response.json()

    return result.get("text", "").strip()


# -----------------------
# 2) ì „ì²˜ë¦¬
# -----------------------
FILLERS = ["ìŒ", "ì–´", "ì €ê¸°ìš”", "ê·¸ë‹ˆê¹Œ", "ì•„ë‹ˆ", "ìŒ...", "ì–´...", "ì €ê¸°"]

def clean_text(text: str) -> str:
    t = text
    for f in FILLERS:
        t = t.replace(f, " ")
    t = re.sub(r"\s+", " ", t)
    t = re.sub(r"[^ê°€-í£0-9a-zA-Z ,.!?]", "", t)
    return t.strip()


# -----------------------
# 3) SBERT ì„ë² ë”©
# -----------------------
embedder = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")

def embed_texts(texts):
    return embedder.encode(texts, show_progress_bar=False)


# -----------------------
# 4) HDBSCAN í´ëŸ¬ìŠ¤í„°ë§
# -----------------------
def cluster_embeddings(embeddings):
    """ì—¬ëŸ¬ ê±´ ì…ë ¥ìš© (batch)"""
    if len(embeddings) < 3:
        # 3ê°œ ë¯¸ë§Œì´ë©´ êµ°ì§‘í™” ë¶ˆê°€ â†’ ëª¨ë‘ -1ë¡œ ì²˜ë¦¬
        return [-1] * len(embeddings)

    scaler = StandardScaler()
    emb_scaled = scaler.fit_transform(embeddings)
    clusterer = hdbscan.HDBSCAN(min_cluster_size=3, metric="euclidean")
    return clusterer.fit_predict(emb_scaled)


# -----------------------
# 5) GPT ìš”ì•½
# -----------------------
def summarize_cluster(texts):
    prompt = f"""
    ì•„ë˜ ë¯¼ì› í…ìŠ¤íŠ¸ë“¤ì˜ ê³µí†µ ì£¼ì œë¥¼ í•œ ì¤„ë¡œ ìš”ì•½í•˜ê³ ,
    ì ì ˆí•œ ì¹´í…Œê³ ë¦¬ ë¼ë²¨ì„ ìƒì„±í•´ì¤˜.

    {texts}
    """

    completion = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}]
    )

    return completion.choices[0].message.content


# -----------------------
# 6) Batch êµ°ì§‘í™” (ì—¬ëŸ¬ ë¯¼ì›)
# -----------------------
def batch_cluster(text_list):
    cleaned = [clean_text(t) for t in text_list]
    embeddings = embed_texts(cleaned)
    labels = cluster_embeddings(embeddings)

    # cluster ID ê¸°ì¤€ìœ¼ë¡œ ë¬¶ê¸°
    clusters = {}
    for text, label in zip(cleaned, labels):
        clusters.setdefault(label, []).append(text)

    results = []
    for cluster_id, items in clusters.items():
        summary = summarize_cluster(items)
        results.append({
            "cluster": int(cluster_id),
            "count": len(items),
            "items": items,
            "summary": summary
        })

    return results


# -----------------------
# 7) ë‹¨ì¼ ë¯¼ì› ì²˜ë¦¬
# -----------------------
def single_complaint(audio_path):
    print("ğŸ“Œ STT ë³€í™˜ ì¤‘...")
    text_raw = clova_stt(audio_path)

    print("ğŸ“Œ ì „ì²˜ë¦¬...")
    text_clean = clean_text(text_raw)

    embeddings = embed_texts([text_clean])

    # ë‹¨ì¼ ë¯¼ì›ì€ cluster=-1 ì²˜ë¦¬
    label = -1

    summary = summarize_cluster([text_clean])

    return {
        "raw": text_raw,
        "clean": text_clean,
        "cluster": label,
        "summary": summary
    }


# -----------------------
# 8) ìë™ ë¶„ê¸° ì²˜ë¦¬ (ë‹¨ì¼ vs ì—¬ëŸ¬ ê±´)
# -----------------------
def process(data, webhook_url=None):
    """
    data:
      - str (ìŒì„± íŒŒì¼ ê²½ë¡œ) â†’ ë‹¨ì¼ ì²˜ë¦¬
      - list[str] (ë¯¼ì› í…ìŠ¤íŠ¸ ëª©ë¡) â†’ batch ì²˜ë¦¬
    """
    # ë‹¨ì¼ ìŒì„± íŒŒì¼ì¸ ê²½ìš°
    if isinstance(data, str):
        result = single_complaint(data)

    # ì—¬ëŸ¬ í…ìŠ¤íŠ¸ (batch)ì¸ ê²½ìš°
    elif isinstance(data, list):
        result = batch_cluster(data)

    else:
        raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° íƒ€ì…ì…ë‹ˆë‹¤.")

    # webhook ë³´ë‚´ê¸°(optional)
    if webhook_url:
        requests.post(webhook_url, json=result)

    return result


# -----------------------
# ì‹¤í–‰ ì˜ˆì‹œ
# -----------------------
if __name__ == "__main__":

    # 1) ë‹¨ì¼ ë¯¼ì› (ìŒì„± íŒŒì¼)
    single_result = process("C:/Users/82104/Downloads/tests.m4a")
    print("ë‹¨ì¼ ë¯¼ì› ê²°ê³¼:", single_result)

    # 2) ì—¬ëŸ¬ ê±´(batch) ë¯¼ì›
    batch_data = [
        "ì§€í•˜ì²  ì†ŒìŒ ë„ˆë¬´ ì‹¬í•´ìš”",
        "ì „ì² ì—ì„œ ê¸°ê³„ìŒì´ ê³„ì† ë‚©ë‹ˆë‹¤",
        "ê³¨ëª© ê°€ë¡œë“±ì´ êº¼ì ¸ ìˆìŠµë‹ˆë‹¤",
        "ê°€ë¡œë“± ê³ ì¥ ë‚¬ì–´ìš”",
        "ë²„ìŠ¤ ì‹œê°„í‘œ ì•Œë ¤ì£¼ì„¸ìš”"
    ]

    batch_result = process(batch_data)
    print("Batch ë¯¼ì› ê²°ê³¼:", batch_result)
import os
import hdbscan
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import StandardScaler
import requests
import openai
import re
from dotenv import load_dotenv

# -----------------------
# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
# -----------------------
load_dotenv()
CLOVA_ID = os.getenv("CLOVA_CLIENT_ID")
CLOVA_SECRET = os.getenv("CLOVA_CLIENT_SECRET")
OPENAI_KEY = os.getenv("OPENAI_API_KEY")

openai.api_key = OPENAI_KEY


# -----------------------
# 1) CLOVA STT
# -----------------------
def clova_stt(audio_path: str) -> str:
    url = "https://naveropenapi.apigw.ntruss.com/recog/v1/stt?lang=Kor"

    headers = {
        "X-NCP-APIGW-API-KEY-ID": CLOVA_ID,
        "X-NCP-APIGW-API-KEY": CLOVA_SECRET,
        "Content-Type": "application/octet-stream",
    }

    with open(audio_path, "rb") as f:
        audio_data = f.read()

    response = requests.post(url, headers=headers, data=audio_data)
    result = response.json()

    return result.get("text", "").strip()


# -----------------------
# 2) ì „ì²˜ë¦¬
# -----------------------
FILLERS = ["ìŒ", "ì–´", "ì €ê¸°ìš”", "ê·¸ë‹ˆê¹Œ", "ì•„ë‹ˆ", "ìŒ...", "ì–´...", "ì €ê¸°"]

def clean_text(text: str) -> str:
    t = text
    for f in FILLERS:
        t = t.replace(f, " ")
    t = re.sub(r"\s+", " ", t)
    t = re.sub(r"[^ê°€-í£0-9a-zA-Z ,.!?]", "", t)
    return t.strip()


# -----------------------
# 3) SBERT ì„ë² ë”©
# -----------------------
embedder = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")

def embed_texts(texts):
    return embedder.encode(texts, show_progress_bar=False)


# -----------------------
# 4) HDBSCAN í´ëŸ¬ìŠ¤í„°ë§
# -----------------------
def cluster_embeddings(embeddings):
    """ì—¬ëŸ¬ ê±´ ì…ë ¥ìš© (batch)"""
    if len(embeddings) < 3:
        # 3ê°œ ë¯¸ë§Œì´ë©´ êµ°ì§‘í™” ë¶ˆê°€ â†’ ëª¨ë‘ -1ë¡œ ì²˜ë¦¬
        return [-1] * len(embeddings)

    scaler = StandardScaler()
    emb_scaled = scaler.fit_transform(embeddings)
    clusterer = hdbscan.HDBSCAN(min_cluster_size=3, metric="euclidean")
    return clusterer.fit_predict(emb_scaled)


# -----------------------
# 5) GPT ìš”ì•½
# -----------------------
def summarize_cluster(texts):
    prompt = f"""
    ì•„ë˜ ë¯¼ì› í…ìŠ¤íŠ¸ë“¤ì˜ ê³µí†µ ì£¼ì œë¥¼ í•œ ì¤„ë¡œ ìš”ì•½í•˜ê³ ,
    ì ì ˆí•œ ì¹´í…Œê³ ë¦¬ ë¼ë²¨ì„ ìƒì„±í•´ì¤˜.

    {texts}
    """

    completion = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}]
    )

    return completion.choices[0].message.content


# -----------------------
# 6) Batch êµ°ì§‘í™” (ì—¬ëŸ¬ ë¯¼ì›)
# -----------------------
def batch_cluster(text_list):
    cleaned = [clean_text(t) for t in text_list]
    embeddings = embed_texts(cleaned)
    labels = cluster_embeddings(embeddings)

    # cluster ID ê¸°ì¤€ìœ¼ë¡œ ë¬¶ê¸°
    clusters = {}
    for text, label in zip(cleaned, labels):
        clusters.setdefault(label, []).append(text)

    results = []
    for cluster_id, items in clusters.items():
        summary = summarize_cluster(items)
        results.append({
            "cluster": int(cluster_id),
            "count": len(items),
            "items": items,
            "summary": summary
        })

    return results


# -----------------------
# 7) ë‹¨ì¼ ë¯¼ì› ì²˜ë¦¬
# -----------------------
def single_complaint(audio_path):
    print("ğŸ“Œ STT ë³€í™˜ ì¤‘...")
    text_raw = clova_stt(audio_path)

    print("ğŸ“Œ ì „ì²˜ë¦¬...")
    text_clean = clean_text(text_raw)

    embeddings = embed_texts([text_clean])

    # ë‹¨ì¼ ë¯¼ì›ì€ cluster=-1 ì²˜ë¦¬
    label = -1

    summary = summarize_cluster([text_clean])

    return {
        "raw": text_raw,
        "clean": text_clean,
        "cluster": label,
        "summary": summary
    }


# -----------------------
# 8) ìë™ ë¶„ê¸° ì²˜ë¦¬ (ë‹¨ì¼ vs ì—¬ëŸ¬ ê±´)
# -----------------------
def process(data, webhook_url=None):
    """
    data:
      - str (ìŒì„± íŒŒì¼ ê²½ë¡œ) â†’ ë‹¨ì¼ ì²˜ë¦¬
      - list[str] (ë¯¼ì› í…ìŠ¤íŠ¸ ëª©ë¡) â†’ batch ì²˜ë¦¬
    """
    # ë‹¨ì¼ ìŒì„± íŒŒì¼ì¸ ê²½ìš°
    if isinstance(data, str):
        result = single_complaint(data)

    # ì—¬ëŸ¬ í…ìŠ¤íŠ¸ (batch)ì¸ ê²½ìš°
    elif isinstance(data, list):
        result = batch_cluster(data)

    else:
        raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° íƒ€ì…ì…ë‹ˆë‹¤.")

    # webhook ë³´ë‚´ê¸°(optional)
    if webhook_url:
        requests.post(webhook_url, json=result)

    return result


# -----------------------
# ì‹¤í–‰ ì˜ˆì‹œ
# -----------------------
if __name__ == "__main__":

    # 1) ë‹¨ì¼ ë¯¼ì› (ìŒì„± íŒŒì¼)
    single_result = process("C:/Users/82104/Downloads/tests.m4a")
    print("ë‹¨ì¼ ë¯¼ì› ê²°ê³¼:", single_result)

    # 2) ì—¬ëŸ¬ ê±´(batch) ë¯¼ì›
    batch_data = [
        "ì§€í•˜ì²  ì†ŒìŒ ë„ˆë¬´ ì‹¬í•´ìš”",
        "ì „ì² ì—ì„œ ê¸°ê³„ìŒì´ ê³„ì† ë‚©ë‹ˆë‹¤",
        "ê³¨ëª© ê°€ë¡œë“±ì´ êº¼ì ¸ ìˆìŠµë‹ˆë‹¤",
        "ê°€ë¡œë“± ê³ ì¥ ë‚¬ì–´ìš”",
        "ë²„ìŠ¤ ì‹œê°„í‘œ ì•Œë ¤ì£¼ì„¸ìš”"
    ]

    batch_result = process(batch_data)
    print("Batch ë¯¼ì› ê²°ê³¼:", batch_result)
